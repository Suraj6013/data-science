{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\suraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\suraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\suraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2: Download necessary resources for NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Document:\n",
      "\n",
      "Natural language processing (NLP) is a field of artificial intelligence (AI) that focuses on \n",
      "the interaction between computers and humans through natural language. The ultimate objective \n",
      "of NLP is to enable computers to understand, interpret, and generate human language in a way \n",
      "that is both meaningful and useful.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load the sample document\n",
    "sample_document = \"\"\"\n",
    "Natural language processing (NLP) is a field of artificial intelligence (AI) that focuses on \n",
    "the interaction between computers and humans through natural language. The ultimate objective \n",
    "of NLP is to enable computers to understand, interpret, and generate human language in a way \n",
    "that is both meaningful and useful.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample Document:\")\n",
    "print(sample_document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.', 'The', 'ultimate', 'objective', 'of', 'NLP', 'is', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'meaningful', 'and', 'useful', '.']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Tokenization\n",
    "tokens = word_tokenize(sample_document)\n",
    "print(\"Tokenization:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging:\n",
      "[('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(', '('), ('AI', 'NNP'), (')', ')'), ('that', 'WDT'), ('focuses', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('interaction', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('humans', 'NNS'), ('through', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.'), ('The', 'DT'), ('ultimate', 'JJ'), ('objective', 'NN'), ('of', 'IN'), ('NLP', 'NNP'), ('is', 'VBZ'), ('to', 'TO'), ('enable', 'JJ'), ('computers', 'NNS'), ('to', 'TO'), ('understand', 'VB'), (',', ','), ('interpret', 'VB'), (',', ','), ('and', 'CC'), ('generate', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('in', 'IN'), ('a', 'DT'), ('way', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('both', 'DT'), ('meaningful', 'JJ'), ('and', 'CC'), ('useful', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"POS Tagging:\")\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after Stopwords Removal:\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'field', 'artificial', 'intelligence', '(', 'AI', ')', 'focuses', 'interaction', 'computers', 'humans', 'natural', 'language', '.', 'ultimate', 'objective', 'NLP', 'enable', 'computers', 'understand', ',', 'interpret', ',', 'generate', 'human', 'language', 'way', 'meaningful', 'useful', '.']\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Stopwords Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(\"Tokens after Stopwords Removal:\")\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens:\n",
      "['natur', 'languag', 'process', '(', 'nlp', ')', 'field', 'artifici', 'intellig', '(', 'ai', ')', 'focus', 'interact', 'comput', 'human', 'natur', 'languag', '.', 'ultim', 'object', 'nlp', 'enabl', 'comput', 'understand', ',', 'interpret', ',', 'gener', 'human', 'languag', 'way', 'meaning', 'use', '.']\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\")\n",
    "print(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens:\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'field', 'artificial', 'intelligence', '(', 'AI', ')', 'focus', 'interaction', 'computer', 'human', 'natural', 'language', '.', 'ultimate', 'objective', 'NLP', 'enable', 'computer', 'understand', ',', 'interpret', ',', 'generate', 'human', 'language', 'way', 'meaningful', 'useful', '.']\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\")\n",
    "print(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Representation:\n",
      "[[0.1132277  0.33968311 0.1132277  0.1132277  0.1132277  0.22645541\n",
      "  0.1132277  0.1132277  0.1132277  0.1132277  0.1132277  0.1132277\n",
      "  0.1132277  0.1132277  0.1132277  0.1132277  0.33968311 0.33968311\n",
      "  0.1132277  0.22645541 0.22645541 0.1132277  0.22645541 0.1132277\n",
      "  0.1132277  0.22645541 0.22645541 0.1132277  0.22645541 0.1132277\n",
      "  0.1132277  0.1132277  0.1132277 ]]\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Calculate Term Frequency and Inverse Document Frequency (TF-IDF)\n",
    "documents = [sample_document]\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"TF-IDF Representation:\")\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
